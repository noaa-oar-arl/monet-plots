{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nScorecard\n=========\n\n**What it's for:**\nThe Scorecard is a tabular visualization that provides a high-level overview of model\nperformance across multiple variables, metrics, lead times, or locations. It uses color-coding\nto highlight areas of strength and weakness.\n\n**When to use:**\nUse this when you have a large number of performance statistics to present simultaneously.\nIt is particularly effective for comparing a new model version against a baseline, or\nfor identifying which meteorological variables or forecast horizons are most problematic.\n\n**How to read:**\n*   **Rows/Columns:** Represent different dimensions of the evaluation (e.g., Variable vs.\n    Forecast Lead Time).\n*   **Cell Color:** Indicates the performance level. Typically, a diverging colormap is\n    used where green represents improvement (or good performance) and red represents\n    degradation (or poor performance) relative to a benchmark.\n*   **Interpretation:** Look for patterns in the colors. For example, a whole row of red\n    might indicate a systemic issue with a specific variable, while a column of red might\n    indicate a drop in performance at a specific lead time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom monet_plots.plots.scorecard import ScorecardPlot\n\n# 1. Prepare sample data in long format\nnp.random.seed(42)  # for reproducibility\n\nvariables = [\"Temperature\", \"Humidity\", \"Wind Speed\", \"Pressure\"]\nlead_times = [\"+06h\", \"+12h\", \"+18h\", \"+24h\", \"+36h\"]\n\ndata_list = []\nfor var in variables:\n    for lt in lead_times:\n        # Simulate a metric value (e.g., RMSE difference from baseline)\n        # Values around 0 mean similar performance, positive means worse, negative means better\n        metric_value = np.random.normal(loc=0, scale=0.5)\n        if var == \"Temperature\" and lt == \"+06h\":\n            metric_value = -1.2  # Example of good performance\n        elif var == \"Pressure\" and lt == \"+36h\":\n            metric_value = 1.5  # Example of poor performance\n\n        data_list.append(\n            {\"Variable\": var, \"Lead Time\": lt, \"Metric Value\": metric_value}\n        )\n\ndf = pd.DataFrame(data_list)\n\n# 2. Initialize and create the plot\nplot = ScorecardPlot(figsize=(10, 7))\nplot.plot(\n    df,\n    x_col=\"Lead Time\",\n    y_col=\"Variable\",\n    val_col=\"Metric Value\",\n    cmap=\"RdYlGn_r\",  # Red-Yellow-Green colormap, reversed so green is good (negative values)\n    center=0,  # Center the colormap at 0\n    linewidths=0.5,  # Add lines between cells\n    linecolor=\"black\",\n)\n\n# 3. Add titles and labels (plot.plot sets default title and labels)\nplot.ax.set_title(\"Model Performance Scorecard (RMSE Difference)\", fontsize=14)\nplot.ax.set_xlabel(\"Forecast Lead Time\")\nplot.ax.set_ylabel(\"Meteorological Variable\")\n\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}